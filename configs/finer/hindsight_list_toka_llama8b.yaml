## FinLoRA Finer â€” HindsightAgent on local JSONL test set

runtime:
  max_envs_to_visit: 250
  max_steps_per_episode: 1
  scores_path: scores.jsonl
  run_validation_at_start: true
  validation_num_workers: 200
  validation_freq: 10
  # Checkpointing
  checkpoint_dir: checkpoints
  checkpoint_every_episodes: 50
  # Use one of the following strategies:
  # - last_n: keeps the last N checkpoints saved every `checkpoint_every_episodes`
  # - top_k_val: keeps the top-K checkpoints by validation mean score (saved on validation)
  checkpoint_strategy: last_n
  checkpoint_keep_last: 20
  checkpoint_on_start: false

train_dataset:
  type: finer
  hf_dataset: "stalaei/finer_v1"
  split: "train_ICL"
  input_field: "context"
  target_field: "target"
  instruction_template: null
  max_samples: 2000
  shuffle: false

validation_dataset:
  type: finer
  hf_dataset: "stalaei/finer_v1"
  split: "val"
  input_field: "context"
  target_field: "target"
  instruction_template: null
  max_samples: 1000
  shuffle: false

agent:
  type: hindsight_agent
  lm_config:
    # Use provider-prefixed model to dispatch to Tokasaurus
    model: "toka:meta-llama/Llama-3.1-8B-Instruct"
    train_temperature: 0.3
    val_temperature: 0.0
    max_output_tokens: 2048
    log_calls: true
    # Tokasaurus-specific fields (present on TokasaurusConfig)
    base_url: "http://localhost:8080"
    stop_sequences: ["FEEDBACK", "OBSERVATION"]
    max_retries: 3
  memory_config:
    _type: history_list
    max_length: 2000
  history_k: null
  system_prompt: "{file:src/data/prompts/finer/system_prompt_brad_magic_with_gpt_fewshots.txt}"

output:
  results_dir: outputs/finer/hindsight_agent/brad_magic_with_progressive_fewshots
  log_level: INFO


