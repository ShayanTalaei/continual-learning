## FinLoRA Finer â€” HistoryAgent on local JSONL test set

runtime:
  max_envs_to_visit: 10000
  max_steps_per_episode: 1
  scores_path: scores.jsonl
  run_validation_at_start: true
  validation_num_workers: 200
  validation_freq: 10
  # Checkpointing
  checkpoint_dir: checkpoints
  checkpoint_every_episodes: 50
  # Use one of the following strategies:
  # - last_n: keeps the last N checkpoints saved every `checkpoint_every_episodes`
  # - top_k_val: keeps the top-K checkpoints by validation mean score (saved on validation)
  checkpoint_strategy: last_n
  checkpoint_keep_last: 20
  checkpoint_on_start: false
  # resume_from: "/matx/u/stalaei/logs/outputs/finer/history_agent/2000_steps_toka_llama3.1_8b_instruct/20251012_141408/checkpoints/ep_000500"
  start_episode_index: 0

train_dataset:
  type: finer
  hf_dataset: "stalaei/finer_test_balanced"
  split: "train"
  input_field: "context"
  target_field: "target"
  instruction_template: null
  max_samples: 2000
  shuffle: false

validation_dataset:
  type: finer
  hf_dataset: "stalaei/finer_test_balanced"
  split: "val"
  input_field: "context"
  target_field: "target"
  instruction_template: null
  max_samples: 1000
  shuffle: false

agent:
  type: history_agent
  lm_config:
    # Use provider-prefixed model to dispatch to Tokasaurus
    model: "toka:meta-llama/Llama-3.1-8B-Instruct"
    temperature: 0.1
    max_output_tokens: 2048
    log_calls: true
    # Tokasaurus-specific fields (present on TokasaurusConfig)
    base_url: "http://localhost:8080"
    stop_sequences: ["FEEDBACK", "OBSERVATION"]
    max_retries: 2
  memory_config:
    _type: history_list
    max_length: 2000
  history_k: null
  system_prompt: "{file:src/data/prompts/finer/system_prompt_simple_thinking.txt}"

output:
  results_dir: outputs/finer/history_agent/2000_steps_toka_llama3.1_8b_instruct
  log_level: INFO


