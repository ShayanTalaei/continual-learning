## OMEGA (allenai/omega-explorative) â€” HistoryAgent

runtime:
  max_envs_to_visit: 500
  max_steps_per_episode: 1
  scores_path: scores.jsonl
  run_validation_at_start: true
  validation_num_workers: 200
  validation_freq: 10

train_dataset:
  hf_dataset: "stalaei/omega-explorative-with-pro-gt-solutions-41-cat"
  split: "train"
  input_field: "messages"
  target_field: "ground_truth"
  instruction_template: null
  max_samples: null
  shuffle: false
  seed: 42
  eval_mode: numeric_tol
  eval_tolerance: 1e-6
  expect_boxed: true
  # Keep only the first N subsets (by first appearance) and first K samples per subset
  num_subsets: 40
  samples_per_subset: 20
  # Controls how incorrect answers are explained
  feedback_type: ground_truth_solution

validation_dataset:
  hf_dataset: "stalaei/omega-explorative-with-pro-gt-solutions-41-cat"
  split: "val"
  input_field: "messages"
  target_field: "ground_truth"
  instruction_template: null
  max_samples: null
  shuffle: false
  seed: 42
  eval_mode: numeric_tol
  eval_tolerance: 1e-6
  expect_boxed: true
  # During validation we can also subset deterministically if desired
  num_subsets: 40
  samples_per_subset: 10
  # During validation, use templated message only
  feedback_type: final_answer

agent:
  type: history_agent
  lm_config:
    model: "gemini-2.0-flash" #"gemini-2.5-flash"
    temperature: 0.0
    max_output_tokens: 8192
    log_calls: true
  memory_config:
    _type: history_list
    max_length: 2000
  history_k: null # this the number previous history items obs, action, feedback collectively, and not their tuples
  system_prompt: "You are a careful math assistant. You will recieve feedback after each action. You should continually learn from the feedback to improve your performance in the subsequent actions. Don't write ACTION: in your answer."

output:
  results_dir: outputs/omega_mixed_subset_41_cat/history_agent/8k_flash_2.0_with_gt_solution_feedback
  log_level: INFO


