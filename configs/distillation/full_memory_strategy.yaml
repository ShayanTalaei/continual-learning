## Test Config for Full Memory Strategy
## This config uses the full_memory strategy to create samples with complete memory
## and observations from a target dataset (validation set)

checkpoint_dir: "/scratch/m000122/stalaei/logs/continual_learning/outputs/finer/history_agent/brad_magic_with_progressive_fewshots/20251020_111212/checkpoints/ep_000250"

# Output directory for generated dataset
output_dir: "/scratch/m000122/stalaei/logs/continual_learning/data/cartridge-oct22_250train_128tokens_sysmem-cache-step700"

# Output format: "jsonl" or "parquet"
output_format: "jsonl"

# Teacher messages text storage
include_teacher_messages_texts: true

# Individual file saving for resume functionality
save_individual_files: true

# Memory formation strategy - using full_memory
strategy:
  name: "full_memory"
  # Path to the memory checkpoint (can be different from checkpoint_dir)
  memory_checkpoint_path: "/scratch/m000122/stalaei/logs/continual_learning/outputs/finer/history_agent/brad_magic_with_progressive_fewshots/20251020_111212/checkpoints/ep_000250"
  # Target dataset configuration (e.g., validation set)
  target_dataset_config:
    type: finer
    hf_dataset: "stalaei/finer_v1"
    split: "val"
    input_field: "context"
    target_field: "target"
    instruction_template: null
    max_samples: null
    shuffle: false
  # Maximum number of samples to use from target dataset
  max_target_samples: 1000

# Agent prompt rendering
agent_type: "history_agent"
system_prompt_override: "{file:src/data/prompts/finer/system_prompt_brad_magic.txt}"

# Language model configuration (Tokasaurus server)
lm_model: "meta-llama/Llama-3.1-8B-Instruct"
lm_base_url: "http://localhost:8096"
lm_temperature: 0.0
lm_max_output_tokens: 2048

# Optional cartridges passed to the LM server
cartridges:
  - id: oct22_250train_128tokens_sysmem-cache-step700
    source: local
    force_redownload: false

# Retry configuration for robust data generation
lm_max_retries: 2

# Optional: capture top-k logprobs for distillation training
top_logprobs: 20

# Enable full sequence tensor data for advanced distillation training
include_full_sequence_data: true

# Sampling parameters - limit to small number for testing
max_samples: null
num_repeat_samples: 4 # -1 for 1 repeatition with no explicit repetition ID

# Parallelism: use single thread for testing
num_threads: 32

# Additional metadata to include in each sample
metadata:
  dataset_version: "full_memory_v1"
  description: "Test dataset using full memory strategy with validation set observations"
