## Test Config for Full Memory Strategy with Shuffling
## This config uses the full_memory strategy with shuffling to create samples with 
## shuffled complete memory and observations from a target dataset (validation set)

# Path to the HistoryAgent checkpoint directory containing memory snapshots
# This is used as a fallback for non-full_memory strategies
checkpoint_dir: "/mnt/data/stalaei/logs/outputs/finer/history_agent/2000_steps_toka_llama3.1_8b_instruct/20251016_070912/checkpoints/ep_000100"

# Output directory for generated dataset
output_dir: "/mnt/data/shayan_memory/finer_train_data_gen_full_memory_shuffled_8x15000"

# Output format: "jsonl" or "parquet"
output_format: "jsonl"

# Teacher messages text storage
include_teacher_messages_texts: true

# Individual file saving for resume functionality
save_individual_files: true

# Memory formation strategy - using full_memory with shuffling
strategy:
  name: "full_memory"
  # Path to the memory checkpoint (can be different from checkpoint_dir)
  memory_checkpoint_path: "/mnt/data/stalaei/logs/outputs/finer/history_agent/2000_steps_toka_llama3.1_8b_instruct/20251016_070912/checkpoints/ep_000100"
  # Target dataset configuration (e.g., validation set)
  target_dataset_config:
    type: finer
    hf_dataset: "stalaei/finer_test_balanced"
    split: "train"
    input_field: "context"
    target_field: "target"
    instruction_template: null
    max_samples: null
    shuffle: false
  # Maximum number of samples to use from target dataset
  max_target_samples: null
  # Optional slicing over the target environments
  start_idx: 100
  end_idx: 15100
  # Shuffling configuration
  do_shuffle: true
  num_shufflings: 8

# Agent prompt rendering
agent_type: "history_agent"
system_prompt_override: "{file:src/data/prompts/finer/system_prompt_brad_magic.txt}"

# Language model configuration (Tokasaurus server)
lm_model: "meta-llama/Llama-3.1-8B-Instruct"
lm_base_url: "http://localhost:8080"
lm_temperature: 0.0
lm_max_output_tokens: 2048

# Retry configuration for robust data generation
lm_max_retries: 2

# Optional: capture top-k logprobs for distillation training
top_logprobs: 20

# Enable full sequence tensor data for advanced distillation training
include_full_sequence_data: true

# Sampling parameters - no repetition, shuffling provides the variation
max_samples: null
num_repeat_samples: -1 # -1 for 1 repetition with no explicit repetition ID

# Parallelism: use single thread for testing
num_threads: 64

# Additional metadata to include in each sample
metadata:
  dataset_version: "full_memory_shuffled_v1"
  description: "Test dataset using full memory strategy with 16 shufflings per problem and validation set observations"
