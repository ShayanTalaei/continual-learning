"""
Convert intermediate dataset format to cartridges-compatible format.

This script reads the intermediate JSONL dataset generated by data_generation.py
and converts it to the cartridges format (parquet with proper logprobs structure).
"""

import sys
import json
import argparse
import os
from pathlib import Path
from typing import List, Dict, Any, Optional
import numpy as np
from transformers import AutoTokenizer
from datasets import load_dataset

# Set up cartridges environment variables BEFORE importing
REPO_ROOT = Path(__file__).parent.parent.parent.parent
CARTRIDGES_DIR = REPO_ROOT / "third_party" / "cartridges"

# Cartridges requires these environment variables
os.environ["CARTRIDGES_DIR"] = str(CARTRIDGES_DIR)
if "CARTRIDGES_OUTPUT_DIR" not in os.environ:
    # Default to ./outputs if not set
    os.environ["CARTRIDGES_OUTPUT_DIR"] = str(REPO_ROOT / "outputs")

# Add third_party/cartridges to path
sys.path.insert(0, str(CARTRIDGES_DIR))

from cartridges.structs import Conversation, write_conversations
from cartridges.clients.base import TopLogprobs, FlatTopLogprobs


def convert_openai_logprobs_to_top_logprobs(
    openai_logprobs: Dict[str, Any],
    tokenizer: AutoTokenizer,
) -> Optional[TopLogprobs]:
    """Convert OpenAI-style logprobs to cartridges TopLogprobs format.
    
    OpenAI format:
    {
        "content": [
            {
                "token": "Hello",
                "logprob": -0.5,
                "top_logprobs": [
                    {"token": "Hello", "logprob": -0.5},
                    {"token": "Hi", "logprob": -1.2},
                    ...
                ]
            },
            ...
        ]
    }
    
    Cartridges format:
    TopLogprobs(
        logprobs: np.ndarray  # shape [num_tokens, num_top_logprobs]
        token_ids: np.ndarray  # shape [num_tokens, num_top_logprobs]
    )
    """
    if openai_logprobs is None:
        return None
    
    content = openai_logprobs.get("content")
    if content is None or len(content) == 0:
        return None
    
    # Determine the number of top logprobs from the first token
    num_top_logprobs = len(content[0].get("top_logprobs", []))
    if num_top_logprobs == 0:
        return None
    
    num_tokens = len(content)
    
    # Initialize arrays
    logprobs_array = np.full((num_tokens, num_top_logprobs), -1000.0, dtype=np.float32)
    token_ids_array = np.full((num_tokens, num_top_logprobs), -1, dtype=np.int32)
    
    # Fill arrays
    for i, token_data in enumerate(content):
        top_logprobs = token_data.get("top_logprobs", [])
        for j, top_item in enumerate(top_logprobs[:num_top_logprobs]):
            token_str = top_item.get("token", "")
            logprob_val = top_item.get("logprob", -1000.0)
            
            # Convert token string to token ID
            try:
                token_id_list = tokenizer.encode(token_str, add_special_tokens=False)
                if len(token_id_list) > 0:
                    token_ids_array[i, j] = token_id_list[0]
                else:
                    token_ids_array[i, j] = -1
            except Exception:
                token_ids_array[i, j] = -1
            
            logprobs_array[i, j] = logprob_val
    
    return TopLogprobs(logprobs=logprobs_array, token_ids=token_ids_array)


def convert_row_to_conversation(
    row: Dict[str, Any],
    tokenizer: AutoTokenizer,
    min_prob_mass: float = 0.99,
) -> Conversation:
    """Convert an intermediate row to a Conversation object.
    
    Args:
        row: Intermediate format row with messages, system_prompt, metadata
        tokenizer: Tokenizer to use for encoding messages
        min_prob_mass: Probability mass threshold for flattening logprobs
        
    Returns:
        Conversation object ready for cartridges training
    """
    messages = []
    
    for msg in row["messages"]:
        # Tokenize the message content to get token_ids
        token_ids = tokenizer.apply_chat_template(
            [{
                "content": msg["content"],
                "role": msg["role"],
            }],
            add_system_message=False,
        )
        
        # Convert logprobs if this is an assistant message with logprobs
        top_logprobs = None
        flat_logprobs = None
        if msg["role"] == "assistant":
            logprobs = msg.get("logprobs")
            if logprobs is not None:
                top_logprobs = convert_openai_logprobs_to_top_logprobs(
                        logprobs,
                        tokenizer
                    )
                if top_logprobs is not None:
                    flat_logprobs = top_logprobs.flatten(threshold=min_prob_mass)
            else:
                raise ValueError(f"[Converter] No logprobs for entry {msg}")
    
        messages.append(
            Conversation.Message(
                role=msg["role"],
                content=msg["content"],
                token_ids=token_ids,
                top_logprobs=flat_logprobs,
            )
        )
    
    return Conversation(
        messages=messages,
        system_prompt=row["system_prompt"],
        metadata=row["metadata"],
        type=row["type"],
    )


def convert_dataset(
    input_source: str,
    output_path: Path,
    model_name: str,
    min_prob_mass: float = 0.99,
    max_samples: Optional[int] = None,
    input_type: str = "local",  # "local" or "huggingface"
    split: str = "train",
):
    """Convert intermediate dataset to cartridges format.
    
    Args:
        input_source: Path to local JSONL file OR HuggingFace repo_id
        output_path: Path to output parquet file
        model_name: Model name for tokenizer (e.g., "meta-llama/Llama-3.1-8B-Instruct")
        min_prob_mass: Probability mass threshold for flattening logprobs
        max_samples: Optional limit on number of samples to convert
        input_type: "local" for local file, "huggingface" for HF dataset
        split: Dataset split to use (for HF datasets)
    """
    print(f"[Converter] Loading tokenizer for {model_name}...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    print(f"[Converter] Tokenizer loaded")
    
    if input_type == "huggingface":
        print(f"[Converter] Loading dataset from HuggingFace: {input_source}")
        dataset = load_dataset(input_source, split=split)
        print(f"[Converter] Loaded {len(dataset)} samples from HuggingFace")
        
        # Convert HF dataset to list of rows
        rows = []
        for i, item in enumerate(dataset):
            if max_samples is not None and i >= max_samples:
                break
            rows.append(item)
    else:
        print(f"[Converter] Reading intermediate dataset from {input_source}...")
        input_path = Path(input_source)
        if not input_path.exists():
            raise FileNotFoundError(f"Input file not found: {input_path}")
        
        rows = []
        with open(input_path, "r") as f:
            for i, line in enumerate(f):
                if max_samples is not None and i >= max_samples:
                    break
                row = json.loads(line.strip())
                rows.append(row)
    
    print(f"[Converter] Loaded {len(rows)} rows")
    
    print(f"[Converter] Converting to cartridges format...")
    conversations = []
    for i, row in enumerate(rows):
        # try:
        convo = convert_row_to_conversation(row, tokenizer, min_prob_mass)
        conversations.append(convo)
        
        if (i + 1) % 100 == 0:
            print(f"[Converter] Converted {i + 1}/{len(rows)} conversations...")
        # except Exception as e:
        #     print(f"[Converter] WARNING: Failed to convert row {i}: {e}")
        #     continue
    
    print(f"[Converter] Successfully converted {len(conversations)} conversations")
    
    print(f"[Converter] Writing to {output_path}...")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    write_conversations(conversations, str(output_path))
    print(f"[Converter] âœ“ Dataset written to {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Convert intermediate dataset format to cartridges-compatible parquet",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Example usage:
  # Convert local JSONL file:
  python src/memory/distillation/convert_to_cartridges.py \\
    --input /path/to/dataset.jsonl \\
    --output /path/to/dataset.parquet \\
    --model meta-llama/Llama-3.1-8B-Instruct

  # Convert HuggingFace dataset:
  python src/memory/distillation/convert_to_cartridges.py \\
    --input stalaei/distillation-dataset-test \\
    --output /path/to/dataset.parquet \\
    --model meta-llama/Llama-3.1-8B-Instruct \\
    --input-type huggingface \\
    --split train
"""
    )
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="Path to intermediate JSONL dataset OR HuggingFace repo_id"
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="Path to output parquet file"
    )
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="Model name for tokenizer (e.g., meta-llama/Llama-3.1-8B-Instruct)"
    )
    parser.add_argument(
        "--input-type",
        type=str,
        choices=["local", "huggingface"],
        default="local",
        help="Input type: 'local' for JSONL file, 'huggingface' for HF dataset (default: local)"
    )
    parser.add_argument(
        "--split",
        type=str,
        default="train",
        help="Dataset split to use (for HuggingFace datasets, default: train)"
    )
    parser.add_argument(
        "--min-prob-mass",
        type=float,
        default=0.99,
        help="Minimum probability mass for flattening logprobs (default: 0.99)"
    )
    parser.add_argument(
        "--max-samples",
        type=int,
        default=None,
        help="Optional limit on number of samples to convert"
    )
    
    args = parser.parse_args()
    
    output_path = Path(args.output)
    
    # Validate input for local files
    if args.input_type == "local":
        input_path = Path(args.input)
        if not input_path.exists():
            print(f"ERROR: Input file not found: {input_path}", file=sys.stderr)
            sys.exit(1)
    
    if not output_path.suffix == ".parquet":
        print("WARNING: Output file should have .parquet extension")
    
    convert_dataset(
        input_source=args.input,
        output_path=output_path,
        model_name=args.model,
        min_prob_mass=args.min_prob_mass,
        max_samples=args.max_samples,
        input_type=args.input_type,
        split=args.split,
    )


if __name__ == "__main__":
    main()

