<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Financial Model Performance</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="../css/tables.css" />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200&icon_names=open_in_new"
    />
    <style>
      .material-symbols-outlined {
        font-size: 14px;
        vertical-align: super;
        color: #4a5568;
      }
      a:hover .material-symbols-outlined {
        color: #2b6cb0;
      }
      .footnote-ref {
        vertical-align: super;
        font-size: 1em;
        color: #4a5568;
      }
      .footnotes {
        margin-top: 0.5rem;
        padding-top: 1rem;
        font-size: 0.875rem;
        color: #6b7280;
      }
      .footnote-item {
        margin-bottom: 0.5rem;
      }
    </style>
  </head>
  <body class="text-gray-900">
    <div class="max-w-7xl mx-auto">
      <p class="text-sm text-gray-500 mb-6">
        Scores: <span class="accuracy-score font-semibold">Accuracy</span> /
        <span class="f1-score font-semibold">F1 Score</span>.
      </p>

      <!-- Container for responsive horizontal scrolling -->
      <div class="overflow-x-auto rounded-lg table-container">
        <table class="w-full text-sm text-left whitespace-nowrap">
          <thead
            class="bg-gray-200 text-xs text-gray-600 uppercase tracking-wider"
          >
            <tr>
              <th scope="col" class="p-2 font-semibold">Model Grouping</th>
              <th scope="col" class="p-2 font-semibold">Model Name</th>
              <th scope="col" class="p-2 font-semibold text-center">
                FPB<a
                  href="https://huggingface.co/datasets/TheFinAI/en-fpb"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </th>
              <th scope="col" class="p-2 font-semibold text-center">
                FiQA SA<a
                  href="https://huggingface.co/datasets/TheFinAI/fiqa-sentiment-classification"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </th>
              <th scope="col" class="p-2 font-semibold text-center">
                TFNS<a
                  href="https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </th>
              <th scope="col" class="p-2 font-semibold text-center">
                NWGI<a
                  href="https://huggingface.co/datasets/TheFinAI/NWGI_test"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </th>
              <th scope="col" class="p-2 font-semibold text-center">
                NER<a
                  href="https://huggingface.co/datasets/FinGPT/fingpt-ner-cls"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </th>
              <th scope="col" class="p-2 font-semibold text-center">
                Headline<a
                  href="https://huggingface.co/datasets/FinGPT/fingpt-headline-cls"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </th>
            </tr>
          </thead>
          <tbody class="divide-y divide-gray-200">
            <!-- Financial Models Group -->
            <tr class="model-group-light">
              <td class="p-2 font-bold align-top" rowspan="2">
                Financial Models
              </td>
              <td class="p-2 font-semibold">
                FinGPT<span class="footnote-ref">†</span
                ><a
                  href="https://openreview.net/pdf?id=FuOMomaQa8"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">86.47</span><br /><span
                  class="f1-score"
                  >0.863</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">81.09</span><br /><span
                  class="f1-score"
                  >0.829</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">88.44</span><br /><span
                  class="f1-score"
                  >0.884</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">56.61</span><br /><span
                  class="f1-score"
                  >0.474</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">-</span><br /><span
                  class="f1-score"
                  >-</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">96.96</span><br /><span
                  class="f1-score"
                  >0.933</span
                >
              </td>
            </tr>
            <tr class="model-group-light">
              <td class="p-2 font-semibold">
                BloombergGPT<span class="footnote-ref">‡</span
                ><a href="https://arxiv.org/abs/2303.17564" target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">51.07</span>
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">75.07</span>
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">-</span>
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">-</span>
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">60.82</span>
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">82.20</span>
              </td>
            </tr>

            <!-- Base Models Group -->
            <tr class="model-group-white">
              <td class="p-2 font-bold align-top" rowspan="5">Base Models</td>
              <td class="p-2 font-semibold">
                Llama 3.1 8B<a
                  href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">68.73</span><br /><span
                  class="f1-score"
                  >0.677</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">46.55</span><br /><span
                  class="f1-score"
                  >0.557</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">69.97</span><br /><span
                  class="f1-score"
                  >0.683</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">43.86</span><br /><span
                  class="f1-score"
                  >0.583</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">48.89</span><br /><span
                  class="f1-score"
                  >0.569</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">45.34</span><br /><span
                  class="f1-score"
                  >0.558</span
                >
              </td>
            </tr>
            <tr class="model-group-white">
              <td class="p-2 font-semibold">
                Llama 3.1 70B<a
                  href="https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">74.50</span><br /><span
                  class="f1-score"
                  >0.736</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">47.27</span><br /><span
                  class="f1-score"
                  >0.565</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">68.42</span><br /><span
                  class="f1-score"
                  >0.686</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">50.14</span><br /><span
                  class="f1-score"
                  >0.596</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">46.28</span><br /><span
                  class="f1-score"
                  >0.454</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">71.68</span><br /><span
                  class="f1-score"
                  >0.729</span
                >
              </td>
            </tr>
            <tr class="model-group-white">
              <td class="p-2 font-semibold">
                DeepSeek V3<a
                  href="https://github.com/deepseek-ai/DeepSeek-V3"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">78.76</span><br /><span
                  class="f1-score"
                  >0.764</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">60.43</span><br /><span
                  class="f1-score"
                  >0.686</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">84.38</span><br /><span
                  class="f1-score"
                  >0.846</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">7.44</span><br /><span
                  class="f1-score"
                  >0.097</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">40.82</span><br /><span
                  class="f1-score"
                  >0.360</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">76.06</span><br /><span
                  class="f1-score"
                  >0.779</span
                >
              </td>
            </tr>
            <tr class="model-group-white">
              <td class="p-2 font-semibold">
                GPT-4o<a
                  href="https://openai.com/index/hello-gpt-4o"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">81.13</span><br /><span
                  class="f1-score"
                  >0.818</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">72.34</span><br /><span
                  class="f1-score"
                  >0.773</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">73.32</span><br /><span
                  class="f1-score"
                  >0.740</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">66.61</span><br /><span
                  class="f1-score"
                  >0.656</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">52.11</span><br /><span
                  class="f1-score"
                  >0.523</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">80.53</span><br /><span
                  class="f1-score"
                  >0.814</span
                >
              </td>
            </tr>
            <tr class="model-group-white">
              <td class="p-2 font-semibold">
                Gemini 2.0 FL<a
                  href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash-lite"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">81.02</span><br /><span
                  class="f1-score"
                  >0.894</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">68.09</span><br /><span
                  class="f1-score"
                  >0.810</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">26.38</span><br /><span
                  class="f1-score"
                  >0.385</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">48.16</span><br /><span
                  class="f1-score"
                  >0.614</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">65.13</span><br /><span
                  class="f1-score"
                  >0.769</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">76.60</span><br /><span
                  class="f1-score"
                  >0.847</span
                >
              </td>
            </tr>

            <!-- Fine-tuned Models Group -->
            <tr class="model-group-light">
              <td class="p-2 font-bold align-top" rowspan="5">
                Fine-tuned Models
              </td>
              <td class="p-2 font-semibold">
                Llama 3.1 8B LoRA<a
                  href="https://huggingface.co/collections/wangd12/finlora-adapters-8bit-quantization-rank-8-684a45430e4d4a8d7ba205a4"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">85.64</span><br /><span
                  class="f1-score"
                  ><b>0.922</b></span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">81.28</span><br /><span
                  class="f1-score"
                  ><b>0.884</b></span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">88.02</span><br /><span
                  class="f1-score"
                  ><b>0.932</b></span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">54.16</span><br /><span
                  class="f1-score"
                  ><b>0.690</b></span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score"><b>98.05</b></span
                ><br /><span class="f1-score"><b>0.981</b></span>
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">84.66</span><br /><span
                  class="f1-score"
                  >0.852</span
                >
              </td>
            </tr>
            <tr class="model-group-light">
              <td class="p-2 font-semibold">
                Llama 3.1 8B QLoRA<a
                  href="https://huggingface.co/collections/wangd12/finlora-adapters-4bit-quantization-rank-4-qlora-68411b488cb0edba3ab6cc07"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">84.16</span><br /><span
                  class="f1-score"
                  >0.909</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">78.30</span><br /><span
                  class="f1-score"
                  >0.874</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">83.84</span><br /><span
                  class="f1-score"
                  >0.910</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">49.96</span><br /><span
                  class="f1-score"
                  >0.645</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">96.63</span><br /><span
                  class="f1-score"
                  >0.966</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">88.03</span><br /><span
                  class="f1-score"
                  >0.886</span
                >
              </td>
            </tr>
            <tr class="model-group-light">
              <td class="p-2 font-semibold">
                Llama 3.1 8B DoRA<a
                  href="https://huggingface.co/collections/wangd12/finlora-adapters-8bit-quantization-rank-8-dora-684390c22fb89bec8c54d4e7"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">81.93</span><br /><span
                  class="f1-score"
                  >0.901</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">78.72</span><br /><span
                  class="f1-score"
                  >0.874</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">59.09</span><br /><span
                  class="f1-score"
                  >0.702</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">19.57</span><br /><span
                  class="f1-score"
                  >0.281</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">71.59</span><br /><span
                  class="f1-score"
                  >0.834</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">64.93</span><br /><span
                  class="f1-score"
                  >0.781</span
                >
              </td>
            </tr>
            <tr class="model-group-light">
              <td class="p-2 font-semibold">
                Llama 3.1 8B rsLoRA<a
                  href="https://huggingface.co/collections/wangd12/finlora-adapters-8bit-quantization-rank-8-rslora-6843a625175b696cbdf8a986"
                  target="_blank"
                  ><span class="material-symbols-outlined">open_in_new</span></a
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">82.84</span><br /><span
                  class="f1-score"
                  >0.853</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">73.19</span><br /><span
                  class="f1-score"
                  >0.806</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">59.51</span><br /><span
                  class="f1-score"
                  >0.655</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">35.80</span><br /><span
                  class="f1-score"
                  >0.464</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">95.92</span><br /><span
                  class="f1-score"
                  >0.963</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">71.75</span><br /><span
                  class="f1-score"
                  >0.828</span
                >
              </td>
            </tr>
            <tr class="model-group-light">
              <td class="p-2 font-semibold">Gemini 2.0 FL N/A</td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score"><b>87.62</b></span
                ><br /><span class="f1-score">0.878</span>
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score"><b>88.09</b></span
                ><br /><span class="f1-score">0.879</span>
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score"><b>89.49</b></span
                ><br /><span class="f1-score">0.896</span>
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score"><b>62.59</b></span
                ><br /><span class="f1-score">0.581</span>
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score">97.29</span><br /><span
                  class="f1-score"
                  >0.973</span
                >
              </td>
              <td class="p-2 text-center data-cell">
                <span class="accuracy-score"><b>97.32</b></span
                ><br /><span class="f1-score"><b>0.973</b></span>
              </td>
            </tr>
          </tbody>
        </table>
      </div>

      <!-- Footnotes -->
      <div class="footnotes">
        <div class="footnote-item">
          <strong>†</strong> FinGPT is based on Llama 2 7B fine-tuned model.
          Results are from the
          <a
            target="_blank"
            href="https://openreview.net/pdf?id=FuOMomaQa8"
            style="text-decoration: underline"
            >paper</a
          >.
        </div>
        <div class="footnote-item">
          <strong>‡</strong> BloombergGPT is a closed model. Results are from
          the
          <a
            target="_blank"
            href="https://arxiv.org/abs/2303.17564"
            style="text-decoration: underline"
            >paper</a
          >.
        </div>
      </div>
    </div>
  </body>
</html>
