{
  "sentiment_llama_3_1_8b_8bits_r8_rslora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/finlora_sentiment_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 8,
    "gradient_accumulation_steps": 2,
    "peft_use_rslora": true
  },
  "headline_llama_3_1_8b_8bits_r8_rslora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/headline_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 8,
    "gradient_accumulation_steps": 2,
    "peft_use_rslora": true
  },
  "ner_llama_3_1_8b_8bits_r8_rslora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/ner_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 8,
    "gradient_accumulation_steps": 2,
    "peft_use_rslora": true
  },
  "finer_llama_3_1_8b_8bits_r8_rslora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/finer_train_batched.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 1,
    "gradient_accumulation_steps": 8,
    "peft_use_rslora": true
  },
  "xbrl_extract_llama_3_1_8b_8bits_r8_rslora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/xbrl_extract_train_batched.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 1,
    "batch_size": 1,
    "gradient_accumulation_steps": 8,
    "peft_use_rslora": true
  },
  "financebench_llama_3_1_8b_8bits_r8_rslora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/financebench_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 1,
    "gradient_accumulation_steps": 2,
    "peft_use_rslora": true
  },
  "formula_llama_3_1_8b_8bits_r8_rslora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/formula_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 1,
    "batch_size": 4,
    "gradient_accumulation_steps": 2,
    "peft_use_rslora": true
  },
  "xbrl_term_llama_3_1_8b_8bits_r8_rslora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/xbrl_term_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 1,
    "batch_size": 4,
    "gradient_accumulation_steps": 2,
    "peft_use_rslora": true
  },

  "sentiment_llama_3_1_8b_8bits_r8_dora_a32": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/finlora_sentiment_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 8,
    "gradient_accumulation_steps": 2,
    "lora_alpha": 32,
    "peft_use_dora": true
  },
  "headline_llama_3_1_8b_8bits_r8_dora_a32": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/headline_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 8,
    "gradient_accumulation_steps": 2,
    "lora_alpha": 32,
    "peft_use_dora": true
  },
  "ner_llama_3_1_8b_8bits_r8_dora_a32": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/ner_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 8,
    "gradient_accumulation_steps": 2,
    "lora_alpha": 32,
    "peft_use_dora": true
  },
  "finer_llama_3_1_8b_8bits_r8_dora_a32": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/finer_train_batched.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 1,
    "gradient_accumulation_steps": 8,
    "lora_alpha": 32,
    "peft_use_dora": true
  },
  "xbrl_extract_llama_3_1_8b_8bits_r8_dora_a32": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/xbrl_extract_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 1,
    "batch_size": 1,
    "gradient_accumulation_steps": 8,
    "lora_alpha": 32,
    "peft_use_dora": true
  },
  "financebench_llama_3_1_8b_8bits_r8_dora_a32": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/financebench_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 1,
    "gradient_accumulation_steps": 2,
    "lora_alpha": 32,
    "peft_use_dora": true
  },
  "formula_llama_3_1_8b_8bits_r8_dora_a32": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/formula_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 1,
    "batch_size": 4,
    "gradient_accumulation_steps": 2,
    "lora_alpha": 32,
    "peft_use_dora": true
  },
  "xbrl_term_llama_3_1_8b_8bits_r8_dora_a32": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/xbrl_term_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 1,
    "batch_size": 4,
    "gradient_accumulation_steps": 2,
    "lora_alpha": 32,
    "peft_use_dora": true
  },

  "sentiment_llama_3_1_8b_8bits_r8_dora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/finlora_sentiment_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 8,
    "gradient_accumulation_steps": 2,
    "peft_use_dora": true
  },
  "headline_llama_3_1_8b_8bits_r8_dora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/headline_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 8,
    "gradient_accumulation_steps": 2,
    "peft_use_dora": true
  },
  "ner_llama_3_1_8b_8bits_r8_dora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/ner_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 8,
    "gradient_accumulation_steps": 2,
    "peft_use_dora": true
  },
  "finer_llama_3_1_8b_8bits_r8_dora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/finer_train_batched.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 1,
    "gradient_accumulation_steps": 8,
    "peft_use_dora": true
  },
  "xbrl_extract_llama_3_1_8b_8bits_r8_dora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/xbrl_extract_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 1,
    "batch_size": 1,
    "gradient_accumulation_steps": 8,
    "peft_use_dora": true
  },
  "financebench_llama_3_1_8b_8bits_r8_dora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/financebench_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 1,
    "gradient_accumulation_steps": 2,
    "peft_use_dora": true
  },
  "formula_llama_3_1_8b_8bits_r8_dora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/formula_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 1,
    "batch_size": 4,
    "gradient_accumulation_steps": 2,
    "peft_use_dora": true
  },
  "xbrl_term_llama_3_1_8b_8bits_r8_dora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/xbrl_term_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 1,
    "batch_size": 4,
    "gradient_accumulation_steps": 2,
    "peft_use_dora": true
  },

  "xbrl_extract_llama_3_1_8b_8bits_r8": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/xbrl_extract_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 1,
    "batch_size": 1,
    "gradient_accumulation_steps": 8
  },
  "xbrl_extract_llama_3_1_8b_4bits_r4": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/xbrl_extract_train.jsonl",
    "lora_r": 4,
    "quant_bits": 4,
    "learning_rate": 0.0001,
    "num_epochs": 1,
    "batch_size": 1,
    "gradient_accumulation_steps": 8
  },
  "finer_llama_3_1_8b_8bits_r8": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/finer_train_batched.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 1,
    "gradient_accumulation_steps": 8
  },
  "financebench_llama_3_1_8b_8bits_r8": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/financebench_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 1,
    "gradient_accumulation_steps": 2
  },
  "formula_llama_3_1_8b_8bits_r8": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/formula_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 1,
    "batch_size": 4,
    "gradient_accumulation_steps": 2
  },
  "xbrl_term_llama_3_1_8b_8bits_r8": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/xbrl_term_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 1,
    "batch_size": 4,
    "gradient_accumulation_steps": 2
  },
  "finer_llama_3_1_8b_4bits_r4": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/finer_train_batched.jsonl",
    "lora_r": 4,
    "quant_bits": 4,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 1,
    "gradient_accumulation_steps": 8
  },
  "sentiment_llama_3_1_8b_4bits_r4": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/finlora_sentiment_train.jsonl",
    "lora_r": 4,
    "quant_bits": 4,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 8,
    "gradient_accumulation_steps": 2
  },
  "financebench_llama_3_1_8b_4bits_r4": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/financebench_train.jsonl",
    "lora_r": 4,
    "quant_bits": 4,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 1,
    "gradient_accumulation_steps": 2
  },
  "formula_llama_3_1_8b_4bits_r4": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/formula_train.jsonl",
    "lora_r": 4,
    "quant_bits": 4,
    "learning_rate": 0.0001,
    "num_epochs": 1,
    "batch_size": 4,
    "gradient_accumulation_steps": 2
  },
  "xbrl_term_llama_3_1_8b_4bits_r4": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/xbrl_term_train.jsonl",
    "lora_r": 4,
    "quant_bits": 4,
    "learning_rate": 0.0001,
    "num_epochs": 1,
    "batch_size": 4,
    "gradient_accumulation_steps": 2
  },
  "cfa_llama_3_1_8b_8bits_r8": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/cfa_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 4,
    "gradient_accumulation_steps": 2
  },
  "cfa_llama_3_1_8b_4bits_r4": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/cfa_train.jsonl",
    "lora_r": 4,
    "quant_bits": 4,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 4,
    "gradient_accumulation_steps": 2
  },
  "cfa_llama_3_1_8b_8bits_r8_dora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/cfa_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 4,
    "gradient_accumulation_steps": 2,
    "peft_use_dora": true
  },
  "cfa_llama_3_1_8b_8bits_r8_rslora": {
    "base_model": "meta-llama/Llama-3.1-8B-Instruct",
    "dataset_path": "../data/train/cfa_train.jsonl",
    "lora_r": 8,
    "quant_bits": 8,
    "learning_rate": 0.0001,
    "num_epochs": 4,
    "batch_size": 4,
    "gradient_accumulation_steps": 2,
    "peft_use_rslora": true
  }
}
